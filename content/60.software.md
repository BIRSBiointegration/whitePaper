## Software strategies to enable analyses of multimodal single cell experiments

### Key questions

* How should multimodal single cell data be managed for interactive and batch analyses?
* What methods will help software developers create scalable solutions for multimodal single cell analysis?
* How can we ensure that visualization methods that are central to multimodal single cell analysis
are usable by researchers with visual impairments?

### Data management strategies

* Abstract data type: "multiassay experiment".  This reflects the idea that each mode will
be characterized by a different collection of features on possibly non-overlapping collections
of samples.  The metadata on features should be clearly and conventionally defined.  For example,
genes and transcripts are enumerated using Ensembl catalog identifiers; regions of accessibility
are defined using genomic coordinates in a clearly specified reference build.  Metadata on
samples must include all relevant information on experimental conditions such as treatment,
protocol, and date of technical processing.

(More fodder-AS)

**Key points:**
1) What do we want to store and share? Data object vs analysis object. How would the design change based on what is stored?
2) Do we need a flexible, universal framework (e.g. MAE) or an experiment class for every possible combination of modalities or technologies?
3) Do we have adequate data representation for all “assays”?

   - Multi-modal single cell data may consist of multi-assay measurements from the same cell (e.g. CITE-seq, sci-CAR) or integration of multi-assay measurements from distinct cells from the same or distinct starting samples. A sample here refers to the biological specimen of origin (tissue A from individual X).
A data container for a multi assay analysis must hold
   1) Assay slots containing variables or features from multiple modalities (e.g. gene expression units from scRNA-seq and protein units from sc-proteomics). In some cases, the feature may be multidimensional (e.g. spatial coordinates, locations of eQTLs).
   2) Observations or cell identities
   3) Metadata for sample of origin for the individual cells, e.g. study, center, phenotype, perturbation.
   4) A map between the different assays to enable analysis

   - The MAE is such a Bioconductor container for overlapping observations, and may serve as a starting point for further expansion. Besides the primary data elements for storing “data objects”, the summarizedExperiment class offers attributes and Methods for storing results of analysis, as an “analysis object”.
      - While common assays such as RNA-seq and ATAC-seq have well-defined data representations (e.g. transcript names), data representation need to be defined newer assays, which may need multiple dimensions for adequate definition (e.g. x, y, z coordinates for images).
      - The observations of different modalities may not be directly comparable (e.g. RNA may be measured from individual cells but spatial transcriptomics may cover a few cells in the matched area).  
      - In the absence of universal standards, the metadata may vary from analysis to analysis.
      - It is crucial that data containers use consistent assay access methods (possibly through methods inheritance. e.g. from `SummarizedExperiment`). This will ensure less redundancy in development process and allow powerful implementation strategies.


(Note, standard BioC container/class terms may not be correctly used. End of fodder - AS)

* Serializations and data access methods for
    * spatial transcriptomics
    * scNMT-seq ...


### Scalability strategies

### Reducing barriers to interpretable visualizations
Color is a powerful data visualization tool that helps representing the different dimensions of our increasingly complex and rich scientific data.
Color vision deficiencies affect a substantial portion of the population.
Therefore,  it is desirable to aim towards presenting scientific information in a manner that is as accessible as possible for all readers. 
Color vision deficiency leads to difficulties in perceiving patterns (the basis for the Ishihara's color vision tests) in multi-colored figures.
In rare cases, the perceived patterns; e.g. in heatmaps and reduced dimension plots, can differ between individuals with normal and color deficient vision.

One strategy to address these issues is to include colorblind friendly visualizations [@https://doi.org/10.1038/nmeth.1618] as a default setting in our visualizations.
Several colorblind-friendly palettes exist (e.g., see R packages [viridis](https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html) and [dittoSeq](https://github.com/dtm2451/dittoSeq)) and can be integrated into data presentation as the default option.
Even with these palettes in place, it is desirable to limit the number (about 8-10 at a maximum) of colors in visualizations.
To reduce the dependence on colors, one solution would be to include additional visual cues to differentiate regions (hatched areas) or cells (point shapes).
Overall, a broader discussion regarding the accessibility of our figures that is not just limited to color vision deficiencies would be greatly beneficial towards improving data accessibility.
Perhaps one tool to address broader accessibility could be the inclusion an "accessibility caption" accompanying figures which "guide" the reader's perception of the images.


[Reference 1: Color Coding](https://www.nature.com/articles/nmeth0810-573)

[Reference 2: Points of View: Color Blindness](https://www.nature.com/articles/nmeth.1618)

[Viridis Color Palettes](https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html)

[An overview of the issues with impaired color perception](https://www.vwvj.be/sites/default/files/zien/zien_-_uit_voorbije_vorming/kleurzinonderzoekextratips.pdf)

[US Government tools for accessibility](https://accessibility.18f.gov/tools/)

### Details of working components -- trimmed

you can interact with underlying data at [google sheet](https://docs.google.com/spreadsheets/d/1tSUQ9iDKqq72TB9G3Cx1evg2sekS-ytx5wRXDv6vxfg/edit?usp=sharing)

|Type|Brief name (link)|Description|
|----|-----------------|-----------|
|Matlab package|[CytoMAP](https://gitlab.com/gernerlab/cytomap)|CytoMAP: A Spatial Analysis Toolbox Reveals Features of Myeloid Cell Organization in Lymphoid Tissues|
|Matlab package|[histoCAT](https://github.com/BodenmillerGroup/histoCAT)|histoCAT: analysis of cell phenotypes and interactions in multiplex image cytometry data|
|Python library|[PyTorch](https://pytorch.org)|General framework for deep learning|
|Python package|[SpaCell](https://github.com/BiomedicalMachineLearning/SpaCell)|SpaCell: integrating tissue morphology and spatial gene expression to predict disease cells|
|Python package|[Scanpy](https://github.com/theislab/scanpy)|Python package for single cell analysis|
|R data class|[MultiAssayExperiment](https://bioconductor.org/packages/MultiAssayExperiment)|unify multiple experiments|
|R data class|[SpatialExperiment](https://github.com/drighelli/SpatialExperiment)|SpatialExperiment: a collection of S4 classes for Spatial Data|
|R package|[Giotto](https://github.com/RubD/Giotto)|Spatial transcriptomics|
|R package|[cytomapper](https://github.com/BodenmillerGroup/cytomapper)|cytomapper: Visualization of highly multiplexed imaging cytometry data in R|
|R package|[Spaniel](https://github.com/RachelQueen1/Spaniel/)|Spaniel: analysis and interactive sharing of Spatial Transcriptomics data|
|R package|[Seurat](https://github.com/satijalab/seurat)|R toolkit for single cell genomics|
|R package|[SpatialLIBD](https://github.com/LieberInstitute/spatialLIBD)|Transcriptome-scale spatial gene expression in the human dorsolateral prefrontal cortex|
|R package|[Cardinal](https://cardinalmsi.org/)|Cardinal: an R package for statistical analysis of mass spectrometry-based imaging experiments|
|R package|[CoGAPS](https://github.com/FertigLab/CoGAPS)|scCoGAPS learns biologically meaningful latent spaces from sparse scRNA-Seq data|
|R package|[projectR](https://github.com/genesofeve/projectR)|ProjectR is a transfer learning framework to rapidly explore latent spaces across independent datasets|
|R package|[SingleCellMultiModal](https://github.com/waldronlab/SingleCellMultiModal)|Serves multiple datasets obtained from GEO and other sources and represents them as MultiAssayExperiment objects|
|R scripts|[SpatialAnalysis](https://github.com/drighelli/SpatialAnalysis)|Scripts for SpatialExperiment usage|
|Self-contained GUI|[ST viewer](https://github.com/jfnavarro/st_viewer)|ST viewer: a tool for analysis and visualization of spatial transcriptomics datasets|
|Shiny app|[Dynverse](https://zouter.shinyapps.io/server/)|A comparison of single-cell trajectory inference methods: towards more accurate and robust tools|
|R package|[mixOmics](https://github.com/mixOmicsTeam/mixOmics)|R toolkit for multivariate analysis of multi-modal data|
|Python package|[totalVI](https://github.com/YosefLab/scVI)|A variational autoencoder (deep learning model) to integrate RNA and protein data from CITE-seq experiments|
|Python web application||[ImJoy](https://imjoy.io/#/)|Deep learning for image analysis|
|Python package|[napari](https://github.com/napari/napari)|Interactive big multi-dimensional 3D image viewer|
|Software|[QuPath](https://qupath.github.io/)|Multiplex whole slide image analysis|
|Python package|[Cytokit](https://github.com/hammerlab/cytokit)|Multiplex whole slide image analysis|
|Python package|[cmIF](https://gitlab.com/engje/cmif)|Multiplex whole slide image analysis|
|Software|[Facetto](https://github.com/kruegert/facetto)|Multiplex whole slide image analysis, not available yet|
|Software, Python based|[CellProfiler](https://cellprofiler.org/)|Image analysis|

Here is the schematic of SpatialExperiment class from Dario Righelli.

<img src="images/SpatialExperimentScheme1.1.png" alt="SpatialExperiment (D. Righelli)" height="400"/>


Here is the schematic of how seqFISH data are stored in the SingleCellMultiModal package from Dario Righelli.

<img src="images/scMultiModal_seqFISH.png" alt="seqFISH (D. Righelli)" height="400"/>

