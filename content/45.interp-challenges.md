<a name="challenges-interp-section"></a>

<!--
Note from KA: I have this feeling that the concept of redundant information across data sets is spread out across most of the sections. Might be worthwhile reorganising the sections, if relevant.
-->


## Challenges for interpretation

The analyses from each hackathon emphasized that regardless of the common difficulties faced by our participants, there is no one method fits all for multi-omics integration. An equally important complement to the diverse computational methods used to solve multi-omics analysis problems rests in the biological interpretation of their solutions, with the notable challenge that the integrated data from these approaches are often of higher dimension than the input datasets. 
For example, low dimensional representation of the results may require additional contiguous data, such as spatial coordinates to capture higher level cellular structure or prognostics (two of our hackathons). Thus, efforts to interpret multi-omics data require standardized vocabulary, benchmarked methods, and abstracted latent variables that can be compared between studies. 

<!--
For example, even abstract lower dimensional representations providing spatial coordinates are often interpreted in terms of their ability to capture higher level cellular structure or prognostics, requiring even further contiguous data than the original high-throughput multi-omics data as input. These approaches also suggest that new measures of the tumor or cell ecosystems of interacting cells are needed because these interactions are fundamental to biological systems. Both the high dimensionality and biological complexity introduce further challenges in understanding and communicating the results from these analyses. 
-->

### Organizing patterns for interpretation

Interpretation hinges on the analysis method selected for a given dataset. Some methods used in the hackathons and summarized in [Table 1](#commontable) aimed to predict a clearly defined outcome, such as recognizing the environment of tumor cells versus that of healthy cells (see [proteomics](#proteomics-section) section). The supervised setting often provides easier interpretations, as one can easily rank the covariates and contiguous data in terms of their predictive potential. 

<!-- One simple delineation between methods used throughout the hackathons and summarized in [Table 1](#commontable), is that some aim to predict a clearly defined outcome at the start of the project, such as recognizing the environment of tumor cells versus that of healthy cells (see [proteomics](#proteomics-section) section). The supervised setting  often provides easier interpretations; one can easily rank the covariates and contiguous data in terms of their predictive potential. -->

However, when data are collected without the availability of a clear response (e.g. survival time, tumor size, cell growth) using multiple
different technologies, data integration requires organizing patterns that enable interpretation. Clustering is often used as one unsupervised method that can use  latent variables - for example using a categorical variable such as cell type which was not directly measured on the data but enables simple interpretations [@doi:10.1016/j.cels.2017.03.006]. Unfortunately, biological phenomenona are often not as clearcut.

<!--In cellular biology,  a favorite such division into clusters is that involved in the definition of cell type [@doi:10.1016/j.cels.2017.03.006], but, unfortunately, biological phenomenona are often not as clearcut. -->

During clustering, overseparating data by forcing the data into types only provides a static description when the variation should often be along a continuum. Indeed, although a latent factor can be a useful first approximation, the development of cells and their fate is a dynamic process. Thus, we recommend referring back to the original data that enabled interpretation of the cell trajectories: in our case, where the underlying latent variable of interest is expressed along a gradient of development (e.g. pseudo-time, disease progression). 

Nonetheless, latent variables represent a rich anchor for many multimodal methods and can often be useful in highlighting what the modalities have in "common" and how they differ, as shown in the [scNMT-seq hackathon](#scnmt-section). Disparate sources of evidence, or in this case, data from different technologies, are more compelling than many replicates of the same technology. Thus, if different technologies allow a consensus on  underlying latent variables, this information is worth retaining. The commonalities are well understood in the case of classical multivariate factor analyses where the data are decomposed into common and unique components [@doi:10.1037/h0069792]. A schematic summary of the different stages in interpretation is provided in Figure {@fig:interpretation}).


<!--
Some scientists may tend to overseparate their data using clustering by forcing the data into types, even when the variation is along a continuum. Early single cell analyses in flow cytometry overused cell gating, resulting in intermediary cells lost during the preprocessing steps. Obviously, eliminating cells in intermediary states to provide clearly delineated inventories of cell types or cells in discrete states only provides a static description and does not enable the prediction or in-depth understanding of transitions between types.
-->

<!--
Although a latent factor can be a useful first approximation, we need to keep in mind that development of cells and their fate is a dynamic process. It is often beneficial to refer back to the original data that enabled interpretation of the cell trajectories: in that case, locally the underlying latent variable of interest is continuous along a gradient of development.
-->

<!--
So far, we have seen two types of latent variables: clusters and a one dimensional continuous "gradient", (pseudo-time, disease progression are two examples of such latent gradients). However the idea of latent variables is a rich anchor for many multimodal methods and can often be useful in highlighting what the modalities have in "common" and how they differ.
The commonalities are well understood in the case of classical multivariate factor analyses where the data are decomposed into "commonalities" and uniqueness components [@doi:10.1037/h0069792].
-->


![](images/interpretability_figure.png){#fig:interpretation width="95%"}

Caption figure: 
**A** Schematic diagram of stages of interpretation and integration of data sources. 
**B** Standards in Geographic Information Systems enable the integration of multiple layers of data. 
**C** Integrative analysis across multiple modes of data results in complementary evidence, allowing stronger conclusions, an instance of Cardinal Newman's principle: *'Supposes a thesis (e.g. the guilt of an accused man) is supported by a great deal of circumstantial evidence of different forms, but in agreement with each other; then even if each piece of evidence is in itself insufficient to produce any strong belief, the thesis is decisively strengthened by their joint effect.'*



### Reasoning by analogy with geospatial problems

Multiple domains of knowledge can be combined easily if there is a common coordinate system, as in geospatial analyses. This is often a goal in multimodal or conjoint analyses, when the first step is to find a common compromise or consensus on which to project each of the individual modalities.
Conjoint analyses also known as STATIS [@doi:10/c8xttz] was a very early multimodal method designed as "PCA of PCAs" where the first step in the analyses was to identify the commonalities between different modalities and define a consensus  onto which the individual data sets were projected [@doi:10.1214/193940307000000455]. STATIS can be considered as an extension of the class of matrix decomposition methods to data cubes.
Many extensions to matrix decompositions have since been designed for multimodal data, [@doi:10.3389/fgene.2019.00627] offers an overview of the relations between many of them.

In both [spatial transcriptomics](#spatial-section) and the [spatial proteomics](#proteomics-section) hackathons, a spatial dimension was already naturally available, where we could leverage spatial statistics methods to quantify spatial effects. In these studies, contiguity and clustering can be tested and easily understood in the spatial context, and layers of information can be mapped to the natural coordinate system in the same way a GIS system incorporates them (Figure {@fig:interpretation}B).

<!-- for an example of the interactions of immune cells in the tumor environment see [@doi:10.1371/journal.pone.0012420]. -->

The spatial coordinate system analogy can be pursued further by finding a "consensus space" that provides a common coordinate system. Thus, by creating an abstract coordinate space, we can leverage methods developed for true spatial co-occurrences, and evalute these co-occurrences in abstract spatial coordinates as an effective strategy for creating layered maps despite the the absence of a physical coordinate system. There are however pitfalls in using very sophisticated dimension reduction techniques which lead to over-interpretation or misinterpretation of spatial relations. One such example is the size and closeness of clusters in t-SNE which do not represent true densities or similarities in the original data.

<!-- Thus, by creating an abstract coordinate space, we can leverage methods developed for true spatial co-occurrences and use them in evaluating co-occurrences in the abstract spatial coordinates, this is an effective strategy for creating layered maps, even in the absence of a physical coordinate system.
There are however pitfalls in using very sophisticated dimension reduction techniques which lead to over-interpretation or misinterpretation of spatial relations, one example is the size and closeness of clusters in t-SNE which do not represent true densities or similarities in the original data.
-->


### Explaining results by linking databases

Figure {@fig:interpretation}A shows how connections to layers of information from external databases can be incorporated into the final output. Real biological understanding is often limited to the integration of this contiguous information that is available from metadata or from exterior sources such as Gene Ontologies, Biomart [@doi:10.1038/nprot.2009.97], Kegg, Human Cell Atlas (HCA) or within software systems (see [software section](#software-section)).

As many methods suffer from identifiability issues, redundant biological knowledge can be enlightening. By providing information on the extreme points in a map or brushing a map with known gene expression features, one can delineate orientations and clusters. As an example, it is only through coloring by CD56 across time that we can see  the dynamics of immune response [@doi:10.3389/fimmu.2020.00714], similar to the principle behind the interactive brushing illustrated in Figure {@fig:interpretation}C.


### Explaining methods

Simulations can often provide effective and transparent communication tools to shed light into complex analytical methods. By generating data from different probabilistic models, we increase our understanding of the methods' limitations including identifiability problems resulting from overparametrized models. More realistic data can also be simulated by adding constraints on the parameters that reduce or eliminate identifiability issues. By using well defined generative processes during data simulation, we can then benchmark methods to clarify what some complex methods do, as we discuss in the [benchmarking](#benchmarking-section) section.

Visualization of step-by-step transformations and optimizations of data also help clarify how certain methods fit models or reduce data dimensionality. These visualizations are often very specialized (e.g. correspondence analyses, goodness of fit qqplots or rootograms,  mean-variance fitting plots), but serve as  intermediary checks to unpack seemingly black boxes analytical processes.

Finally, spanning all of these interpretation challenges is a central communication barriers between data analysts and the community of practitionners who do not have the same vocabulary or background. Many tools are used as black boxes where users do not have a clear understanding on the statistical or mathematical principles underpinning the methods. A clear glossary of terms, and how we are using those terms is crucial to improve communication. For example, many synonyms for multimodal data exist and some have nuances, as we have collated in [Table 2 ](#glossary). Understanding the relation between methods described by different teams is essential. Data scientist often try to organize the methods first, thus it is useful to create a dichotomy of methods and their underlying properties for our collaborators.



<!--
### Disparate sources of evidence are more compelling than more of the same
Following [Cardinal Newman's principle](https://www.encyclopedia.com/people/philosophy-and-religion/roman-catholic-and-orthodox-churches-general-biographies/john-henry-newman)^[Supposes a thesis (e.g. the guilt of an accused man) is supported by a great deal of circumstantial evidence of different forms, but in agreement with each other; then even if each piece of evidence is in itself insufficient to produce any strong belief, the thesis is decisively strengthened by their joint effect.] disparate sources of evidence, or in this case data from different technologies, are more compelling than many replicates of the same technology.
Thus, if different technologies allow a consensus on  underlying latent variables, this information is worth retaining.
-->
<!--
### Explaining results to biologists through generative models and simulations (ex: Factor Analysis, Hierarchical models).
Several difficulties arise when explaining summaries and conclusions. Problems encountered include non-identifiability of models
or non-sufficiency of summaries, simulations can often provide effective communication tools.
One can often generate data from different probabilistic models and show that the methods cannot differentiate between the generation processes, this is the identifiability problems that most overparametrized models lead to.
Added constraints on the parameters can often
be integrated into the analyses to make them more realistic and reduce if not eliminate the identifiability issues.
-->

<!--
### Meaningful Interpretation by linking in databases
In the right side of Figure {@fig:interpretation}A we show how connections to layers of information from outside databases can be incorporated into the final output. Real biological understanding is often subordinated to the integration
 of this contiguous information.
Either from the metadata already available in the multiassay containers as for instance in the [MultiAssayExperiment package](https://bioconductor.org/packages/release/bioc/html/MultiAssayExperiment.html) or from exterior sources such as Gene Ontologies, Biomart [@doi:10.1038/nprot.2009.97], Kegg, Human Cell Atlas (HCA) or other sources often available through links provided within systems like bioconductor [@https://bioconductor.org].
Redundant biological knowledge is often enlightening,
as many methods suffer from identifiability issues (ie in a gradient, the direction of the direction is unknown).
By providing information on the extreme points in a map
or brushing a map with known gene expression features
one can delineate orientations and clusters.
As an example, it is only through  coloring by CD56 across time that we can see  the dynamics of immune response [@doi:10.3389/fimmu.2020.00714] (similar to the principle behind the interactive brushing illustrated in Figure {@fig:interpretation}C).
-->
<!--
### Visualization tools for interpretation and communication to biologists
An example of effective visual interpretation tools is interactive brushing of UMAP [@doi:10.1038/nbt.4314]
 plot, see Figure {@fig:interpretation}C.
-->
<!--
### Interpretation for data scientists reading the methods sections requires a good understanding of the building blocks
Spanning all of these interpretation challenges is a further central communication barriers within the community of data scientists, computer scientists and computational biologists ie communicating about methods within a community of practitionners who do not have the same vocabulary or background.
Many tools are used as black boxes and users
don't know or agree on what exactly the methods are doing (MOFA and tSNE are examples).
The first step in unblinding these black boxes used as methodology shortcuts is to have a clear glossary of terms and how we are using them.
Many synonyms for multimodal data exist and some have nuances, see the table we have compiled (ref: Table1).
Understanding the relation between methods developed by different teams is essential and we often try to organize the methods first, thus it is useful to create a dichotomy of methods and their underlying properties.
A very useful tool for making methodological black boxes more transparent are simulated data.
These can follow benchmark methods such as those presented in [benchmarking](#benchmarking-section) and use well defined generative processes to clarify what some complex methods do.
Visualization of the data, following the step by step transformations and optimizations of data representations also help  clarify how certain methods fit models or  compress and reduce data dimensionality.
These visualizations are often very specialized (think for instance, correspondence analyses, goodness of fit plots like qqplots or rootograms or mean-variance fitting).
These intermediary plots don't usually end up in the main text of final biological publications and serve as intermediary checks to unpack the black boxes.
-->


<!--
Footnote:
Cardinal Newman wrote  **The Grammar of Assent.** and cited in  [Bruno de Finetti, Volume 1, 1974 Theory of Probability]:
*Supposes a thesis (e.g. the guilt of an accused man) is supported by a great deal of circumstantial evidence of different forms, but in agreement with each other; then even if each piece of evidence is in itself insufficient to produce any strong belief, the thesis is decisively strengthened by their joint effect.*
-->

